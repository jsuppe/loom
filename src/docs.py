"""
Loom Document Generator - Maintains REQUIREMENTS.md and TEST_SPEC.md
"""

from pathlib import Path
from datetime import datetime, timezone
from typing import List, Dict, Optional, Tuple, Set

from store import LoomStore, Requirement
from testspec import TestSpec, TestSpecStore


def generate_requirements_doc(store: LoomStore, output_dir: Path, private_ids: Set[str] = None, public_mode: bool = False) -> Path:
    """Generate REQUIREMENTS.md from stored requirements.
    
    Args:
        store: LoomStore instance
        output_dir: Where to write the file
        private_ids: Set of requirement IDs to exclude (if public_mode)
        public_mode: If True, exclude private requirements
    """
    private_ids = private_ids or set()
    reqs = store.list_requirements(include_superseded=False)
    superseded = [r for r in store.list_requirements(include_superseded=True) if r.superseded_at]
    
    if public_mode:
        reqs = [r for r in reqs if r.id not in private_ids]
        superseded = [r for r in superseded if r.id not in private_ids]
    
    # Group by domain
    by_domain: Dict[str, List[Requirement]] = {}
    for req in reqs:
        by_domain.setdefault(req.domain, []).append(req)
    
    lines = [
        "# Requirements Document",
        "",
        f"*Auto-generated by Loom â€” {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M UTC')}*",
        "",
        f"**Active Requirements:** {len(reqs)}  ",
        f"**Superseded:** {len(superseded)}",
        "",
        "---",
        "",
    ]
    
    # Domain order
    domain_order = ["terminology", "behavior", "ui", "data", "architecture"]
    domains = sorted(by_domain.keys(), key=lambda d: domain_order.index(d) if d in domain_order else 99)
    
    for domain in domains:
        domain_reqs = by_domain[domain]
        lines.append(f"## {domain.title()}")
        lines.append("")
        for req in sorted(domain_reqs, key=lambda r: r.timestamp):
            lines.append(f"### {req.id}")
            lines.append("")
            lines.append(f"> {req.value}")
            lines.append("")
            lines.append(f"- **Source:** {req.source_session}")
            lines.append(f"- **Added:** {req.timestamp[:10]}")
            lines.append("")
        lines.append("---")
        lines.append("")
    
    # Superseded section
    if superseded:
        lines.append("## Superseded Requirements")
        lines.append("")
        lines.append("*These requirements have been replaced by newer versions.*")
        lines.append("")
        for req in superseded:
            lines.append(f"- ~~{req.id}: {req.value}~~ (superseded {req.superseded_at[:10]})")
        lines.append("")
    
    output_path = output_dir / "REQUIREMENTS.md"
    output_path.write_text("\n".join(lines))
    return output_path


def generate_test_spec_doc(store: LoomStore, output_dir: Path, specs: Dict[str, TestSpec] = None, private_ids: Set[str] = None, public_mode: bool = False) -> Path:
    """Generate TEST_SPEC.md from requirements and test specifications.
    
    Args:
        store: LoomStore instance
        output_dir: Where to write the file
        specs: Dict of req_id -> TestSpec
        private_ids: Set of requirement IDs to exclude (if public_mode)
        public_mode: If True, exclude private requirements
    """
    private_ids = private_ids or set()
    reqs = store.list_requirements(include_superseded=False)
    specs = specs or {}
    
    if public_mode:
        reqs = [r for r in reqs if r.id not in private_ids]
    
    lines = [
        "# Test Specification",
        "",
        f"*Auto-generated by Loom â€” {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M UTC')}*",
        "",
        "This document describes how to verify each requirement.",
        "",
        "| Status | Meaning |",
        "|--------|---------|",
        "| âœ… | Has test specification |",
        "| âš ï¸ | Needs test specification |",
        "| ðŸ”„ | Requirement changed, test needs review |",
        "",
        "---",
        "",
    ]
    
    # Group by domain
    by_domain: Dict[str, List[Requirement]] = {}
    for req in reqs:
        by_domain.setdefault(req.domain, []).append(req)
    
    domain_order = ["terminology", "behavior", "ui", "data", "architecture"]
    domains = sorted(by_domain.keys(), key=lambda d: domain_order.index(d) if d in domain_order else 99)
    
    needs_spec = 0
    has_spec = 0
    
    for domain in domains:
        domain_reqs = by_domain[domain]
        lines.append(f"## {domain.title()}")
        lines.append("")
        
        for req in sorted(domain_reqs, key=lambda r: r.timestamp):
            spec = specs.get(req.id)
            
            if spec:
                has_spec += 1
                status = "âœ…"
                lines.append(f"### {status} {req.id}")
                lines.append("")
                lines.append(f"**Requirement:** {req.value}")
                lines.append("")
                lines.append(f"**Test:** {spec.description}")
                lines.append("")
                if spec.steps:
                    lines.append("**Steps:**")
                    for i, step in enumerate(spec.steps, 1):
                        lines.append(f"{i}. {step}")
                    lines.append("")
                lines.append(f"**Expected:** {spec.expected}")
                lines.append("")
                if spec.automated:
                    lines.append("*Automated: Yes*")
                if spec.last_verified:
                    lines.append(f"*Last verified: {spec.last_verified}*")
                lines.append("")
            else:
                needs_spec += 1
                status = "âš ï¸"
                lines.append(f"### {status} {req.id}")
                lines.append("")
                lines.append(f"**Requirement:** {req.value}")
                lines.append("")
                lines.append("**Test:** *Needs specification*")
                lines.append("")
                lines.append("```")
                lines.append("TODO: Define test steps and expected outcome")
                lines.append("```")
                lines.append("")
        
        lines.append("---")
        lines.append("")
    
    # Summary
    lines.insert(8, f"**Coverage:** {has_spec}/{has_spec + needs_spec} requirements have test specs")
    lines.insert(9, "")
    
    output_path = output_dir / "TEST_SPEC.md"
    output_path.write_text("\n".join(lines))
    return output_path


def check_conflicts(store: LoomStore, new_req: Requirement, get_embedding_fn=None, threshold: float = 0.85) -> List[Dict]:
    """
    Check if a new requirement conflicts with existing ones.
    
    Uses semantic similarity to find potential conflicts.
    Returns list of potential conflicts with similarity scores.
    """
    conflicts = []
    
    existing = store.list_requirements(include_superseded=False)
    
    # If we have embedding function, use semantic search
    if get_embedding_fn:
        new_embedding = get_embedding_fn(new_req.value)
        similar = store.search_requirements(new_embedding, n=5)
        
        for match in similar:
            req = match["requirement"]
            distance = match.get("distance", 1.0)
            
            # ChromaDB returns L2 distance, lower = more similar
            # Convert to similarity (rough approximation)
            similarity = max(0, 1 - (distance / 2))
            
            if req.id != new_req.id and similarity > threshold:
                # High similarity in same domain = likely conflict
                if req.domain == new_req.domain:
                    conflicts.append({
                        "existing": req,
                        "similarity": similarity,
                        "reason": f"High semantic similarity ({similarity:.0%}) in same domain"
                    })
                elif similarity > 0.9:
                    # Very high similarity across domains
                    conflicts.append({
                        "existing": req,
                        "similarity": similarity,
                        "reason": f"Very high semantic similarity ({similarity:.0%})"
                    })
    
    # Also check for keyword-based conflicts
    for req in existing:
        if req.id == new_req.id:
            continue
        if any(c["existing"].id == req.id for c in conflicts):
            continue  # Already found via semantic search
            
        # Same domain with significant word overlap
        if req.domain == new_req.domain:
            new_lower = new_req.value.lower()
            old_lower = req.value.lower()
            
            new_words = set(new_lower.split())
            old_words = set(old_lower.split())
            overlap = new_words & old_words
            
            # Exclude common words
            stopwords = {"the", "a", "an", "is", "are", "should", "be", "to", "for", "with", "and", "or"}
            meaningful_overlap = overlap - stopwords
            
            if len(meaningful_overlap) >= 3:
                conflicts.append({
                    "existing": req,
                    "overlap": meaningful_overlap,
                    "reason": f"Overlapping terms: {', '.join(list(meaningful_overlap)[:5])}"
                })
    
    return conflicts


def analyze_test_impact(store: LoomStore, changed_req: Requirement, specs: Dict[str, TestSpec]) -> List[str]:
    """
    Analyze which tests are impacted by a requirement change.
    
    Returns list of affected test IDs.
    """
    affected = []
    
    # Direct impact - test for this requirement
    if changed_req.id in specs:
        affected.append(changed_req.id)
    
    # Indirect impact - tests that depend on related requirements
    # (This would use the implementation links to find related code/tests)
    
    return affected
